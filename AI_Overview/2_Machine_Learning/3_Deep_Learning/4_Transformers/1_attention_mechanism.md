# Attention Mechanism 注意力機制

如果 RNN 是逐字閱讀，那麼 Attention 就如同 **一目十行**。它徹底改變了 AI 處理資訊的方式，從 **序列處理** 轉向了 **平行全域處理**。

核心概念是 **關聯性 Contextual Relationship**：
> 當我在看這個字的時候，我應該同時將注意力放在句子的其他哪些部分？

## 為什麼需要 Attention？

在翻譯 "The animal didn't cross the street because **it** was too tired" 這句話時，人類很自然知道 **it** 指的是 **animal** 而不是 **street**。
傳統 RNN 很難建立這種長距離的關聯，因為 **it** 和 **animal** 中間隔了很多字。
Self-Attention 讓 **it** 這個字能夠直接與句中所有的字進行互動，並發現它與 **animal** 的關聯性最強。

## 運作原理 Query Key Value

Attention 機制借用了資料庫檢索的概念。每個字都會產生三個向量：

### Query 查詢向量
**我在找什麼？**
這代表當前這個字去 "詢問" 其他字的意圖。

### Key 索引向量
**我是什麼？**
這代表每個字用來被別人搜尋的標籤。

### Value 內容向量
**我包含什麼資訊？**
這代表每個字實際攜帶的語義內容。

### 運算過程
- **比對:** 拿當前字的 **Query** 去跟所有字的 **Key** 做內積運算，算出 **注意力分數 Attention Score**。分數越高代表越相關。
- **加權:** 將這些分數透過 **Softmax** 轉化為機率，總和為 1。
- **聚合:** 根據這些機率，將所有字的 **Value** 加權總和，形成當前這個字的新表示。

## 多頭注意力 Multi-Head Attention

為了捕捉不同層面的關聯，我們會同時使用多組 Q K V 進行運算。
- Head 1 可能專注於 **語法結構**，動詞找主詞。
- Head 2 可能專注於 **指代關係**，it 找 animal。
- Head 3 可能專注於 **時態對應**。

最後將這些不同觀點的結果拼接起來，就得到對這句話最完整的理解。

## 總結

Attention 機制拋棄了 RNN 的循環結構，帶來了兩大革命：
- **平行運算:** 不用等上一個字算完才能算下一個字，速度極快。
- **全域視野:** 不管距離多遠，任何兩個字之間的關聯都能一步到位。
