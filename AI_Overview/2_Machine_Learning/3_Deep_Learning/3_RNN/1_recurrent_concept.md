# Recurrent Neural Network 循環神經網路

如果說 CNN 是用來看圖的眼睛，那麼 RNN 就是用來閱讀的 **大腦**。它賦予了 AI 最重要的人類特質：**記憶 Memory**。

核心概念是 **序列處理 Sequence Processing**：
> 今天的決策不只取決於今天的輸入，還取決於 **昨天的記憶**。

## 為什麼需要 RNN？

傳統的神經網路 DNN 和 CNN 都是 **無記憶的 Stateless**。
- **輸入:** 這張照片是貓嗎？
- **輸出:** 是。
- **下一張圖:** 這張照片是狗嗎？
- **問題:** 模型完全不記得上一張看過什麼。它無法理解電影的情節，也無法聽懂一句完整的話。

RNN 引入了 **迴圈 Loop** 的結構，將上一個時間點 $t-1$ 的運算結果 **Hidden State**，傳遞給下一個時間點 $t$ 當作輸入。

## 運作原理

想像你在閱讀這句話：
> 我討厭吃蘋果，但我喜歡吃香蕉，因為香蕉比較甜。

當你讀到最後一個字 **甜** 的時候，你的腦中不再只是看見 **甜** 這個字，而是包含了前面所有的上下文資訊 香蕉。

RNN 的數學公式：
```
New State = Function(Input + Old State)
```
- **Input:** 當下看到的字。
- **Old State:** 此刻之前的短期記憶。

## 應用場景

只要數據有 **時間順序** 或 **前後因果** 關係，就是 RNN 的戰場：

- **自然語言處理 NLP:** 機器翻譯 Google Translate、聊天機器人 ChatGPT。
- **語音辨識:** Siri 聽寫。
- **時間序列預測:** 股票價格預測、氣溫預測。
- **音樂生成:** 自動作曲。

## 致命缺陷

標準的 RNN 有一個嚴重的問題：**金魚腦**。
由於 **梯度消失 Vanishing Gradient** 問題，RNN 很難記住太久以前的資訊。
- **短句:** 雲是 色的。很容易，看前一個字就知道是白。
- **長文:** 我在法國長大... 中間講了五千字廢話 ...所以我說一口流利的 語。RNN 早就忘記開頭的法國了。

這個限制催生了更強大的架構：**LSTM** 與 **Transformer**。
