# LSTM Spec: Long Short-Term Memory

這是 RNN 的升級版，專門設計來解決 **金魚腦** 問題。

核心概念是 **閥門機制 Gating Mechanism**：
> 給神經網路裝上 **開關**，讓它擁有 **主動選擇** 該記住什麼、該遺忘什麼的能力。

## 核心架構

標準的 RNN 只有一個簡單的迴圈，而 LSTM 的內部結構非常複雜，包含四個互動的神經網路層。

### 關鍵元件 Cell State 用語

**Cell State 記憶單元 $C_t$**
這是一條貫穿所有時間點的 **高速公路**。資訊可以沿著這條路暢通無阻地流動，只有少量的線性運算。這保證了梯度即使經過很長的路徑也不會消失。

### 三大閥門 The Three Gates

LSTM 透過三個閥門來控制這條高速公路的資訊進出：

#### 1. 遺忘閥 Forget Gate
**決定要丟掉什麼舊資訊。**
- **輸入:** 昨天的記憶 + 今天的輸入。
- **動作:** 輸出一個 0 到 1 的數值。
- **意義:** 0 代表 **完全遺忘**，1 代表 **完全保留**。
- **範例:** 讀到「他吃飽了」之後，就可以遺忘「他肚子餓」這個舊狀態。

#### 2. 輸入閥 Input Gate
**決定要儲存什麼新資訊。**
- **動作:** 判斷今天的資訊哪些是重要的，並將其加入 Cell State。
- **範例:** 讀到「現在是晴天」，將「晴天」寫入記憶。

#### 3. 輸出閥 Output Gate
**決定當下要輸出什麼。**
- **動作:** 根據當前的 Cell State，決定要讓外界看到什麼。
- **意義:** 記憶裡可能有很多東西，但我現在只需要講重點。

## 為什麼能解決梯度消失？

在標準 RNN 中，資訊每經過一次傳遞就像乘法一樣 $0.9 \times 0.9 \times 0.9$，十次之後就趨近於 0。
但在 LSTM 中，Cell State 的更新主要是透過 **加法** $C_t = C_{t-1} + New$。
**加法運算** 讓梯度能夠穩定地向後傳遞，不會隨著時間指數衰減。這讓 LSTM 能夠記住幾百甚至幾千步以前的資訊。

## 總結

LSTM 雖然參數多、運算慢，但它讓 AI 第一次擁有了 **長期記憶** 的能力。這也是為什麼在 Transformer 出現之前，LSTM 統治了 NLP 領域長達數年。
