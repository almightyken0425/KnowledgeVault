# Diffusion Model 擴散模型

這是讓 GAN 走下神壇的新王者，也是 Midjourney 與 Stable Diffusion 背後的魔法核心。

核心概念是 **破壞與重建 Destruction and Reconstruction**：
> 先把一幅畫完全毀掉變成雜訊，然後訓練 AI 學會如何把這些雜訊一點一滴地還原成畫作。

## 運作原理

擴散模型包含兩個過程：

### 前向擴散 Forward Diffusion 加噪
**目標:** 破壞資料。
- 我們在真實圖片上，一步一步地加入高斯雜訊 Gaussian Noise。
- 經過幾百步之後，這張圖片會變成一張完全隨機的雜訊圖 Pure Noise，看不出原本是什麼。
- 這個過程是 **已知的**，不需要訓練 AI，我們只是依公式加雜訊而已。

### 反向擴散 Reverse Diffusion 去噪
**目標:** 訓練 AI 進行修復。
- 這是 AI 要學的部分。我們丟給它一張充滿雜訊的圖，以及它原本比較清楚的樣子，上一各時間點的圖。
- 告訴 AI：請預測並減去這一層雜訊。
- 如果 AI 學會了這一步，它就能從一張完全空白的雜訊圖開始，一步一步 **去噪 Denoising**，最終變出一張清晰的貓。

## 為什麼比 GAN 強？

-   **訓練穩定:** 沒有警察與小偷的勾心鬥角，目標函數非常單純，就是預測雜訊，所以很難訓練失敗。
-   **多樣性:** 不會發生 模式崩潰，因為每次從不同的隨機雜訊開始，去噪出來的結果都會不一樣。
-   **可控性:** 我們可以透過 文字引導 Text Guidance CLIP 來控制去噪的方向，告訴它：請往一隻在太空中的貓的方向去噪。

## 代價

天下沒有免費的午餐。GAN 生成圖片只需要跑一次神經網路，速度極快。
Diffusion Model 生成圖片需要跑幾十次甚至上百次去噪步驟，速度較慢。但這換來的是無與倫比的畫質與細節。
