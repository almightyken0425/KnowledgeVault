# Feature Engineering 特徵工程

這是傳統機器學習 Machine Learning 中最耗時也最關鍵的步驟。許多資料科學家甚至認為：資料與特徵決定了機器學習的上限，而模型演算法只是逼近這個上限而已。

核心概念是 **人工萃取 Manual Extraction**：
> 將原始數據 Raw Data 轉換為更能代表潛在問題的特徵 Features，讓演算法更容易理解。

## 為什麼需要特徵工程？

傳統演算法如 SVM 或決策樹，對於 **非結構化數據** 理解能力很差。

### 範例 1：房價預測
- **原始數據:** 台北市信義路五段 7 號
- **機器看不懂:** 這只是一串文字字串。
- **特徵工程:**
-   轉換為經緯度。
-   計算 距離最近捷運站公尺數。
-   計算 附近 500 公尺內便利商店數量。
    - **結果:** 機器看懂了這些數字與房價的正相關性。

### 範例 2：垃圾郵件偵測
- **原始數據:** 這一封 Email 的完整內文。
- **機器看不懂:** 這是一篇長文章。
- **特徵工程:**
-   計算單字 Free 出現的次數。
-   計算單字 Buy 出現的次數。
-   檢查是否包含大量驚嘆號。
    - **結果:** 機器根據這些統計數字判斷是否為垃圾信。

## 與 Deep Learning 的分水嶺

這正是 **Deep Learning** 崛起的原因。

| 特性         | Classical ML                               | Deep Learning                         |
| :----------- | :----------------------------------------- | :------------------------------------ |
| **特徵來源** | **人工定義** 需要領域專家 Domain Knowledge | **自動學習** 模型自行從數據中尋找規律 |
| **擅長資料** | **結構化數據** 表格 CSV Excel              | **感知數據** 圖片 聲音 文字           |
| **工作流程** | 特徵工程 -> 丟入模型                       | 端對端學習 End-to-End Learning        |

### 深度學習的優勢
在深度學習中，我們不再告訴電腦 貓有三角形的耳朵。我們直接丟入一萬張貓的照片，神經網路的第一層可能會學會識別 線條，第二層學會 形狀，第三層學會 五官。

這個 **自動特徵萃取 Automated Feature Extraction** 的能力，是 AI 在 2012 年後爆發式成長的關鍵。

## 常見特徵工程技術

- **獨熱編碼 One-Hot Encoding:** 將類別標籤 蘋果 香蕉 轉為 0 1 向量。
- **標準化 Normalization:** 將身高跟體重縮放到 0 到 1 之間，避免單位影響計算。
- **降維 Dimensionality Reduction:** 使用 PCA 等技術去除不重要的雜訊特徵。
