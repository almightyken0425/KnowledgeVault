# Q-Learning Q-Learning 演算法

這是強化學習中最經典、最基礎的入門演算法。它的名字 **Q** 代表 **Quality 品質**，意即衡量在某個狀態下，採取某個行動的好壞程度。

核心概念是 **價值表 Q-Table**：
> 建立一個巨大的作弊小抄，記錄在每一個情況下，做什麼動作可以拿到最高分。

## Q-Table 結構

想像一個 Excel 表格：
- **列 Rows:** 代表所有的 **State 狀態**。例如 迷宮中的每一個格子。
- **行 Columns:** 代表所有的 **Action 行動**。例如 上 下 左 右。
- **單元格 Cell:** 儲存 **Q-Value**。代表在該狀態下採取該行動，預期未來能拿到的總分。

### 視覺化範例

假設是一個 2x2 的迷宮，目標在右下角。

| State 狀態 | Action: Up | Action: Down      | Action: Left | Action: Right     |
| :--------- | :--------- | :---------------- | :----------- | :---------------- |
| **(0, 0)** | -10 撞牆   | **+5** 往終點靠近 | -10 撞牆     | **+5** 往終點靠近 |
| **(0, 1)** | -10 撞牆   | **+50** 到達終點  | -5 遠離終點  | -10 撞牆          |

**決策邏輯:** Agent 查表發現，在狀態 (0, 0) 時，往下走或往右走分數最高，所以它會選擇這兩個動作之一。

## 學習公式 The Bellman Equation

Q-Learning 如何填寫這個表格？它使用 **貝爾曼方程式** 來更新分數。

### 概念化公式
```
新 Q 值 = 舊 Q 值 + 學習率 * ( 真實獎勵 + 對未來的預期 - 舊 Q 值 )
```

1.  **真實獎勵:** 這一布走下去，環境立刻給你的分數 R。
2.  **對未來的預期:** 假設下一布我也選這最好的走法，未來最多還能拿幾分？
3.  **折扣因子 Gamma:** 未來的分數要打折。今天的 100 元比明年的 100 元值錢。

## 訓練流程

1.  **初始化:** 建立一個 Q-Table，所有分數通常設為 0。
2.  **迴圈 Loop:**
    - **探索策略:** 決定要亂走還是查表。通常使用 **Epsilon-Greedy** 策略，即 90% 查表，10% 亂走以探索新路徑。
    - **執行:** 做出動作，獲得 Reward，進入下一個 State。
    - **更新:** 套用公式更新 Q-Table 中剛才那一格的分數。
3.  **收斂:** 重複千萬次後，Q-Table 的分數會固定下來，這就是 Agent 學到的 **最佳策略 Optimal Policy**。

## 局限性

Q-Learning 需要把所有狀態列成表格。
- **小問題:** 踩地雷、井字遊戲。狀態很少，Q-Table 很完美。
- **大問題:** 圍棋、星海爭霸。狀態比宇宙原子還多，表格記憶體永遠塞不下。

**解決方案:** 這就是 **Deep Q-Network DQN** 出場的時候了。它用 **神經網路** 來取代這張表格，讓神經網路去 **預測** Q-Value，這就是 Deep Learning 與 Reinforcement Learning 的結合。
